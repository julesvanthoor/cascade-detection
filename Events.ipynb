{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-15T22:47:32.004Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_level_events(event_data, segment):\n",
    "    \"\"\"\n",
    "    Computes all segment level events given an event data and segment name.\n",
    "\n",
    "    :param event_data: event data used (Performance_Spectrum.Eventdata)\n",
    "    :param segment: name of segment to compute events for\n",
    "    :return: pandas Dataframe containing all segment level events for 'segment' in 'event_data'\n",
    "    \"\"\"\n",
    "    data = event_data.pf.copy()\n",
    "    data = data[data[\"segment_name\"] == segment]\n",
    "    data[\"case_id\"] = data[\"case_id\"].astype(int)\n",
    "    data[\"start_time\"] = data[\"start_time\"].apply(lambda hours: (\n",
    "        event_data.first + datetime.timedelta(days=np.floor(hours / 24), hours=(hours % 24))).strftime(\"%Y-%m-%d\"))\n",
    "    data[\"end_time\"] = data[\"end_time\"].apply(lambda hours: (\n",
    "        event_data.first + datetime.timedelta(days=np.floor(hours / 24), hours=(hours % 24))).strftime(\"%Y-%m-%d\"))\n",
    "    data[\"duration\"] = pd.to_datetime(\n",
    "        data[\"end_time\"], format='%Y-%m-%d') - pd.to_datetime(data[\"start_time\"], format='%Y-%m-%d')\n",
    "    data[\"duration\"] = data[\"duration\"].apply(lambda td: td.days)\n",
    "    \n",
    "    data[\"start_resource\"] = data[\"start_org:resource\"].str.split(\"_\").str[1]\n",
    "    data[\"start_resource\"] = data[\"start_resource\"].astype(int)\n",
    "    data[\"end_resource\"] = data[\"end_org:resource\"].str.split(\"_\").str[1]\n",
    "    data[\"end_resource\"] = data[\"end_resource\"].astype(int)\n",
    "    data[\"resources\"] = list(zip(data[\"start_resource\"], data[\"end_resource\"]))\n",
    "    \n",
    "    data = data[[\"segment_id\", \"segment_name\", \"start_time\", \"end_time\", \"duration\", \"case_id\", \"resources\"]]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:46.837889Z",
     "start_time": "2021-05-29T18:44:46.823923Z"
    }
   },
   "outputs": [],
   "source": [
    "def system_level_events_delayed(segment_level_events, treshold_occurence=1, treshold_duration=0):\n",
    "    \"\"\"\n",
    "    Computes all delayed system level events (same start and end date) from the segment level events given.\n",
    "\n",
    "    :param segment_level_events: pandas DataFrame containing all segment level events\n",
    "    :param treshold_occurence: a treshold for the minimal number of cases that must be in the system level events\n",
    "    :param treshold_duration: a treshold for the minimal duration of the segments in the system level events\n",
    "    :return: pandas Dataframe containing all computed system level events, filtered on the 2 tresholds\n",
    "    \"\"\"\n",
    "    data = segment_level_events.copy()\n",
    "    data = data.groupby(['segment_name', 'start_time', 'end_time', 'duration'])[['case_id', 'segment_id', 'resources']].agg(list).reset_index()\n",
    "    data = data.rename(columns={'case_id': 'cases', 'segment_id': 'segments'})\n",
    "    \n",
    "    if data.empty:\n",
    "        return data\n",
    "    \n",
    "    data['nr_cases'] = data['cases'].str.len()\n",
    "    \n",
    "    data[\"start_users\"] = data[\"resources\"].apply(lambda x: [*set(i for i, _ in x)])\n",
    "    data[\"end_users\"] = data[\"resources\"].apply(lambda x: [*set(i for _, i in x)])\n",
    "\n",
    "    data = data[data[\"duration\"] >= treshold_duration]\n",
    "    data = data[data['nr_cases'] >= treshold_occurence]\n",
    "    \n",
    "    data = data[[\"segment_name\", \"start_time\", \"end_time\", \"duration\", \"nr_cases\", \"start_users\", \"end_users\", \"cases\", \"segments\"]]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:46.853843Z",
     "start_time": "2021-05-29T18:44:46.839880Z"
    }
   },
   "outputs": [],
   "source": [
    "def system_level_events_batching_on_end(segment_level_events, treshold_occurence=1):\n",
    "    \"\"\"\n",
    "    Computes all batching on end system level events with the same end date from the segment level events given.\n",
    "\n",
    "    :param segment_level_events: pandas DataFrame containing all segment level events\n",
    "    :param treshold_occurence: a treshold for the minimal number of cases that must be in the system level events\n",
    "    :return: pandas Dataframe containing all computed batching on end system level events, filtered on the 2 tresholds\n",
    "    \"\"\"\n",
    "    data = segment_level_events.copy()\n",
    "    \n",
    "    if data.empty:\n",
    "        return data\n",
    "    \n",
    "    data[\"startpoints\"] = list(zip(data.start_time, data.duration, data.case_id))\n",
    "    data = data.groupby(['segment_name', 'end_time'])[['startpoints', 'segment_id', 'resources']].agg(list).reset_index() \n",
    "    \n",
    "    if data.empty:\n",
    "        return data\n",
    "      \n",
    "    data = data.rename(columns={\"segment_id\": \"segments\"})\n",
    "    data[\"cases\"] = data[\"startpoints\"].apply(lambda x: [i for _, _, i in x])\n",
    "    data[\"nr_cases\"] = data[\"cases\"].str.len()\n",
    "    data = data[data[\"nr_cases\"] > treshold_occurence]\n",
    "    \n",
    "    data[\"start_users\"] = data[\"resources\"].apply(lambda x: [*set(i for i, _ in x)])\n",
    "    data[\"end_users\"] = data[\"resources\"].apply(lambda x: [*set(i for _, i in x)])  \n",
    "    \n",
    "    data = data[[\"segment_name\", \"end_time\", \"nr_cases\", \"start_users\", \"end_users\", \"cases\", \"segments\", \"startpoints\"]]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:46.868803Z",
     "start_time": "2021-05-29T18:44:46.854851Z"
    }
   },
   "outputs": [],
   "source": [
    "def system_level_events_batching_on_start(segment_level_events, treshold_occurence=1):\n",
    "    \"\"\"\n",
    "    Computes all batching on start system level events with the same start date from the segment level events given.\n",
    "\n",
    "    :param segment_level_events: pandas DataFrame containing all segment level events\n",
    "    :param treshold_occurence: a treshold for the minimal number of cases that must be in the system level events\n",
    "    :return: pandas Dataframe containing all computed batching on start system level events, filtered on the 2 tresholds\n",
    "    \"\"\"\n",
    "    data = segment_level_events.copy()\n",
    "    \n",
    "    if data.empty:\n",
    "        return data\n",
    "    \n",
    "    data[\"endpoints\"] = list(zip(data.end_time, data.duration, data.case_id))\n",
    "    data = data.groupby(['segment_name', 'start_time'])[\n",
    "        ['endpoints', 'segment_id', 'resources']].agg(list).reset_index() \n",
    "    \n",
    "    if data.empty:\n",
    "        return data\n",
    "     \n",
    "    data = data.rename(columns={\"segment_id\": \"segments\"})\n",
    "    data[\"cases\"] = data[\"endpoints\"].apply(lambda x: [i for _, _, i in x])\n",
    "    data[\"nr_cases\"] = data[\"cases\"].str.len()\n",
    "    data = data[data[\"nr_cases\"] > treshold_occurence]\n",
    "    \n",
    "    data[\"start_users\"] = data[\"resources\"].apply(lambda x: [*set(i for i, _ in x)])\n",
    "    data[\"end_users\"] = data[\"resources\"].apply(lambda x: [*set(i for _, i in x)])\n",
    "    \n",
    "    data = data[[\"segment_name\", \"start_time\", \"nr_cases\", \"start_users\", \"end_users\", \"cases\", \"segments\", \"endpoints\"]]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:46.883787Z",
     "start_time": "2021-05-29T18:44:46.869801Z"
    }
   },
   "outputs": [],
   "source": [
    "def system_level_events_high_workload(segment_level_events, treshold_workload=1, same_user=True):\n",
    "    \"\"\"\n",
    "    Computes all high workload system level events with the same end date from the segment level events given.\n",
    "\n",
    "    :param segment_level_events: pandas DataFrame containing all segment level events\n",
    "    :param treshold_workload: a treshold for the minimal workload per system level event\n",
    "    :return: pandas Dataframe containing all computed high workload system level events, filtered on the 2 tresholds\n",
    "    \"\"\"\n",
    "    data = segment_level_events.copy()\n",
    "    \n",
    "    if data.empty:\n",
    "        return data\n",
    "    \n",
    "    data[\"startpoints\"] = list(zip(data.start_time, data.duration, data.case_id))\n",
    "    data = data.groupby(['segment_name', 'end_time'])[['startpoints', 'segment_id', 'resources']].agg(list).reset_index()  \n",
    "    \n",
    "    if data.empty:\n",
    "        return data\n",
    "      \n",
    "    data = data.rename(columns={\"segment_id\": \"segments\"})  \n",
    "    data[\"cases\"] = data[\"startpoints\"].apply(lambda x: [i for _, _, i in x])\n",
    "    data[\"nr_cases\"] = data[\"cases\"].str.len()\n",
    "    \n",
    "    data[\"start_users\"] = data[\"resources\"].apply(lambda x: [*set(i for i, _ in x)])\n",
    "    data[\"end_users\"] = data[\"resources\"].apply(lambda x: [*set(i for _, i in x)])\n",
    "    data[\"ratio_start_end\"] = data[\"start_users\"].str.len() / data[\"end_users\"].str.len()\n",
    "    data[\"ratio_workload\"] = data[\"nr_cases\"] / data[\"end_users\"].str.len()\n",
    "    \n",
    "    if same_user:\n",
    "        data = data[data[\"ratio_workload\"] > treshold_workload] \n",
    "    else:\n",
    "        data = data[data[\"ratio_start_end\"] > treshold_workload]\n",
    "\n",
    "    data = data[[\"segment_name\", \"end_time\", \"nr_cases\", \"ratio_workload\", \"ratio_start_end\", \"start_users\", \"end_users\", \"cases\", \"segments\", \"startpoints\"]]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:46.930965Z",
     "start_time": "2021-05-29T18:44:46.917672Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_events(events):\n",
    "    data = events.copy()\n",
    "    \n",
    "    base = data[(data[\"type\"] != \"BEN\") & (data[\"type\"] != \"BST\")]\n",
    "\n",
    "    bst = data[data[\"type\"] == \"BST\"]\n",
    "    ben = data[data[\"type\"] == \"BEN\"]\n",
    "    bat = data[(data[\"type\"] == \"BST\") | (data[\"type\"] == \"BEN\")]\n",
    "\n",
    "    bat = bat.drop_duplicates(subset=[\"start_activity\", \"end_activity\", \"start\", \"end\"]).reset_index()\n",
    "    \n",
    "    for i, row in bat.iterrows():\n",
    "        if ((row[\"start\"][0] == row[\"start\"][1]) & (row[\"end\"][0] == row[\"end\"][1])):\n",
    "            bat.at[i, \"type\"] = \"BAT\"\n",
    "    \n",
    "    data = pd.concat([base, bat]).reset_index()\n",
    "    data = data[[\"start_activity\", \"end_activity\", \"start\", \"end\", \"type\", \"info\", \"label\", \"cases\"]]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_level_events(event_log, segments, tresholds, event_type, ext_label, jenks_treshold=63):\n",
    "    \"\"\"\n",
    "    Acts as a wrapper around system level events to prepare them fore cascade detection\n",
    "\n",
    "    :param event_log: event log used (Performance_Spectrum.EventLog)\n",
    "    :param segments: name of segments to compute events for\n",
    "    :param tresholds: tresholds for the events\n",
    "    :return: pandas Dataframe containing all computed system level events\n",
    "    \"\"\"\n",
    "    \n",
    "    sys_events = []\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        seg_events = segment_level_events(event_log, segment)\n",
    "        \n",
    "        if event_type == \"batching_end\" or event_type == \"workload\" or event_type == \"workload_hand\":\n",
    "            if event_type == \"batching_end\":\n",
    "                events = system_level_events_batching_on_end(seg_events, max(jenks_treshold, tresholds[i]))\n",
    "            elif event_type == \"workload\":\n",
    "                events = system_level_events_high_workload(seg_events, tresholds[i])       \n",
    "            elif event_type == \"workload_hand\":\n",
    "                events = system_level_events_high_workload(seg_events, tresholds[i], same_user=False)\n",
    "            \n",
    "            if not events.empty:\n",
    "                if event_type == \"batching_end\":\n",
    "                    events[\"info\"] = events[\"cases\"].str.len()\n",
    "                if event_type == \"workload\":\n",
    "                    events[\"info\"] = events[\"ratio_workload\"]\n",
    "                if event_type == \"workload_hand\":\n",
    "                    events[\"info\"] = events[\"ratio_start_end\"]  \n",
    "            \n",
    "                # Date related operations\n",
    "                dates = events[\"startpoints\"].apply(lambda x: [i for i, _, _ in x])\n",
    "                datetimes = dates.apply(lambda x: [datetime.datetime.strptime(date, \"%Y-%m-%d\") for date in x])\n",
    "                firstdates = datetimes.apply(lambda x: min(x))\n",
    "                lastdates = datetimes.apply(lambda x: max(x))\n",
    "                enddates = events[\"end_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").strftime(\"%y%m%d\"))\n",
    "                events[\"start_date\"] = list(zip(firstdates.apply(lambda x:(x.strftime(\"%y%m%d\"))), \n",
    "                                           lastdates.apply(lambda x:(x.strftime(\"%y%m%d\")))))\n",
    "                events[\"end_date\"] = list(zip(events[\"end_time\"], events[\"end_time\"]))\n",
    "                firstdates = firstdates - FIRST\n",
    "                lastdates = lastdates - FIRST\n",
    "\n",
    "                end = events[\"end_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "                end = end - FIRST\n",
    "                events[\"date_label\"] = enddates.astype(str)         \n",
    "            \n",
    "                events[\"start\"] = list(zip((firstdates.dt.days * 24 + round(firstdates.dt.seconds / 3600)).astype(int), \n",
    "                                           (lastdates.dt.days * 24 + round(lastdates.dt.seconds / 3600)).astype(int)))\n",
    "                events[\"end\"] = list(zip((end.dt.days * 24 + round(end.dt.seconds / 3600)).astype(int), \n",
    "                                         (end.dt.days * 24 + round(end.dt.seconds / 3600)).astype(int)))\n",
    "        elif event_type == \"batching_start\":\n",
    "            events = system_level_events_batching_on_start(seg_events, max(jenks_treshold, tresholds[i]))\n",
    "            \n",
    "            if not events.empty:\n",
    "                events[\"info\"] = events[\"cases\"].str.len()\n",
    "            \n",
    "                # Date related operations\n",
    "                dates = events[\"endpoints\"].apply(lambda x: [i for i, _, _ in x])\n",
    "                datetimes = dates.apply(lambda x: [datetime.datetime.strptime(date, \"%Y-%m-%d\") for date in x])\n",
    "                firstdates = datetimes.apply(lambda x: min(x))\n",
    "                lastdates = datetimes.apply(lambda x: max(x))\n",
    "                startdates = events[\"start_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").strftime(\"%y%m%d\"))\n",
    "                events[\"end_date\"] = list(zip(firstdates.apply(lambda x:(x.strftime(\"%Y-%m-%d\"))), \n",
    "                                           lastdates.apply(lambda x:(x.strftime(\"%Y-%m-%d\")))))\n",
    "                events[\"start_date\"] = list(zip(events[\"start_time\"], events[\"start_time\"]))\n",
    "\n",
    "                firstdates = firstdates - FIRST\n",
    "                lastdates = lastdates - FIRST\n",
    "\n",
    "                start = events[\"start_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "                start = start - FIRST\n",
    "\n",
    "                events[\"date_label\"] = startdates.astype(str)\n",
    "            \n",
    "                events[\"start\"] = list(zip((start.dt.days * 24 + round(start.dt.seconds / 3600)).astype(int), \n",
    "                                           (start.dt.days * 24 + round(start.dt.seconds / 3600)).astype(int)))\n",
    "\n",
    "                events[\"end\"] = list(zip((firstdates.dt.days * 24 + round(firstdates.dt.seconds / 3600)).astype(int), \n",
    "                                         (lastdates.dt.days * 24 + round(lastdates.dt.seconds / 3600)).astype(int)))\n",
    "        elif event_type == \"delayed\":\n",
    "            events = system_level_events_delayed(seg_events, 10, tresholds[i])\n",
    "            \n",
    "            if not events.empty:\n",
    "                events[\"info\"] = events[\"duration\"]\n",
    "\n",
    "                # Date related operations\n",
    "                events[\"start_date\"] = list(zip(events[\"start_time\"], events[\"start_time\"]))\n",
    "                events[\"end_date\"] = list(zip(events[\"end_time\"], events[\"end_time\"]))\n",
    "                startdates = events[\"start_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").strftime(\"%y%m%d\"))\n",
    "                enddates = events[\"end_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").strftime(\"%y%m%d\"))\n",
    "                start = events[\"start_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "                start = start - FIRST\n",
    "                end = events[\"end_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "                end = end - FIRST\n",
    "\n",
    "                events[\"date_label\"] = startdates.astype(str) + \"-\" + enddates.astype(str)\n",
    "\n",
    "                if not events.empty:\n",
    "                    events[\"start\"] = list(zip((start.dt.days * 24 + round(start.dt.seconds / 3600)).astype(int), \n",
    "                                               (start.dt.days * 24 + round(start.dt.seconds / 3600)).astype(int)))\n",
    "                    events[\"end\"] = list(zip((end.dt.days * 24 + round(end.dt.seconds / 3600)).astype(int), \n",
    "                                             (end.dt.days * 24 + round(end.dt.seconds / 3600)).astype(int)))\n",
    "        if not events.empty: \n",
    "            sys_events.append(events)\n",
    "    \n",
    "    if sys_events:\n",
    "        data = pd.concat(sys_events)\n",
    "        temp = data[\"segment_name\"].str.split(\" - \", n=1, expand=True)\n",
    "        data[\"start_activity\"] = temp[0].replace(Short.ACTIVITY)\n",
    "        data[\"end_activity\"] = temp[1].replace(Short.ACTIVITY)\n",
    "        data[\"type\"] = Short.TYPE[event_type]\n",
    "        \n",
    "        if ext_label:\n",
    "            data[\"label\"] = data[\"start_activity\"].astype(str) + \"|\" + data[\"end_activity\"].astype(str) + \" \" + data[\"date_label\"] + \" \" + data[\"type\"] \n",
    "        else:\n",
    "            data[\"label\"] = data[\"start_activity\"].astype(str) + \"|\" + data[\"end_activity\"].astype(str) + \"|\" + data[\"type\"]\n",
    "    \n",
    "    data = data[[\"start_activity\", \"end_activity\", \"start\", \"end\", \"type\", \"info\", \"label\", \"cases\"]]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:46.946188Z",
     "start_time": "2021-05-29T18:44:46.931935Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_system_level_events(event_logs, tresholds, ext_label=True):\n",
    "    events_dict = {}\n",
    "    for i, log in enumerate(event_logs):\n",
    "        print(\"computing events for log:\", log[1])\n",
    "        batching_end = system_level_events(log[0], Segments.SEGMENTS_COMPLETE, tresholds[\"batching_end\"][i], \"batching_end\", ext_label)\n",
    "        batching_start = system_level_events(log[0], Segments.SEGMENTS_COMPLETE, tresholds[\"batching_start\"][i], \"batching_start\", ext_label)\n",
    "        high_workload = system_level_events(log[0], Segments.SEGMENTS_HIGH_WORKLOAD, tresholds[\"high_workload\"][i], \"workload\", ext_label)\n",
    "        high_workload_hand = system_level_events(log[0], Segments.SEGMENTS_HIGH_WORKLOAD_HAND, tresholds[\"high_workload_hand\"][i], \"workload_hand\", ext_label)\n",
    "        delayed = system_level_events(log[0], Segments.SEGMENTS_COMPLETE, tresholds[\"delayed\"], \"delayed\", ext_label)\n",
    "    \n",
    "        events = pd.concat([delayed, batching_end, batching_start, high_workload, high_workload_hand])\n",
    "        events = remove_duplicate_events(events)\n",
    "        events[\"id\"] = range(len(events))\n",
    "        events = events[[\"id\", \"start_activity\", \"end_activity\", \"start\", \"end\", \"type\", \"info\", \"label\", \"cases\"]]\n",
    "        events = rearrange_events(events)\n",
    "        events_dict[log[1]] = events\n",
    "    \n",
    "    save_system_level_events(events_dict)\n",
    "    \n",
    "    return events_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_events(events):\n",
    "    segments = Segments.SEGMENTS_CASCADE0 + Segments.SEGMENTS_CASCADE1 + Segments.SEGMENTS_CASCADE2\n",
    "    data = filter_system_level_events(events, segments)\n",
    "    ordered_list = [(Short.ACTIVITY[segment.split(\" - \")[0]], Short.ACTIVITY[segment.split(\" - \")[1]]) for segment in segments]\n",
    "    data[\"segment\"] = list(zip(data[\"start_activity\"], data[\"end_activity\"]))\n",
    "    data = data.sort_values('segment', key=lambda x: x.map({v:k for k, v in enumerate(ordered_list)}))\n",
    "    data[\"id\"] = range(len(data))\n",
    "    data = data.reset_index()\n",
    "    data = data[[\"id\", \"start_activity\", \"end_activity\", \"start\", \"end\", \"type\", \"info\", \"label\", \"cases\"]]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:46.961836Z",
     "start_time": "2021-05-29T18:44:46.947163Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_system_level_events(events):\n",
    "    with open('output/dumps/events', 'wb+') as output:\n",
    "        pickle.dump(events, output, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:46.977087Z",
     "start_time": "2021-05-29T18:44:46.962833Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_system_level_events():\n",
    "    return pickle.load(open('output/dumps/events', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:46.992359Z",
     "start_time": "2021-05-29T18:44:46.977984Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_system_level_events(events, segments):\n",
    "    data = events.copy()\n",
    "    \n",
    "    data[\"segment\"] = list(zip(data[\"start_activity\"], data[\"end_activity\"]))\n",
    "    lookup_segments = [(Short.ACTIVITY[segment.split(\" - \")[0]], Short.ACTIVITY[segment.split(\" - \")[1]]) for segment in segments]\n",
    "    data = data[data[\"segment\"].isin(lookup_segments)]\n",
    "    \n",
    "    data = data[[\"id\", \"start_activity\", \"end_activity\", \"start\", \"end\", \"type\", \"info\", \"label\", \"cases\"]]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.007491Z",
     "start_time": "2021-05-29T18:44:46.993261Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_adjacency_list(system_level_events, criteria, case_overlap):\n",
    "    data = system_level_events.copy()\n",
    "\n",
    "    if criteria == \"loose\":\n",
    "        data[\"adjacency_list\"] = data.apply(\n",
    "        lambda x: [\n",
    "            row[\"id\"]\n",
    "            for _, row in data[(x[\"end_activity\"] == data[\"start_activity\"])].iterrows()\n",
    "            if (\n",
    "                (\n",
    "                    len(set(row[\"cases\"]).intersection(x[\"cases\"])) >= case_overlap\n",
    "                ) and (\n",
    "                    (x[\"end\"][0] <= row[\"start\"][0] <= x[\"end\"][1]) \n",
    "                    or \n",
    "                    (row[\"start\"][0] <= x[\"end\"][0] <= row[\"start\"][1])\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        axis=1,\n",
    "        )\n",
    "    elif criteria == \"strict\":\n",
    "        data[\"adjacency_list\"] = data.apply(\n",
    "        lambda x: [\n",
    "            row[\"id\"]\n",
    "            for _, row in data[(x[\"end_activity\"] == data[\"start_activity\"])].iterrows()\n",
    "            if (\n",
    "                (\n",
    "                    len(set(row[\"cases\"]).intersection(x[\"cases\"])) >= case_overlap\n",
    "                ) and (\n",
    "                    (x[\"end\"][0] <= row[\"start\"][0] <= row[\"start\"][1] <= x[\"end\"][1]) \n",
    "                    or \n",
    "                    (row[\"start\"][0] <= x[\"end\"][0] <= x[\"end\"][1] <= row[\"start\"][1])\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        axis=1,\n",
    "        )\n",
    "    elif criteria == \"very_strict\":\n",
    "        data[\"adjacency_list\"] = data.apply(\n",
    "        lambda x: [\n",
    "            row[\"id\"]\n",
    "            for _, row in data[(x[\"end_activity\"] == data[\"start_activity\"])].iterrows()\n",
    "            if (\n",
    "                (\n",
    "                    len(set(row[\"cases\"]).intersection(x[\"cases\"])) >= math.floor(min(len(row[\"cases\"])/2, len(x[\"cases\"])/2))\n",
    "                ) and (\n",
    "                    (x[\"end\"][0] <= row[\"start\"][0] <= row[\"start\"][1] <= x[\"end\"][1]) \n",
    "                    or \n",
    "                    (row[\"start\"][0] <= x[\"end\"][0] <= x[\"end\"][1] <= row[\"start\"][1])\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        axis=1,\n",
    "        )\n",
    "    data = data[[\"id\", \"start_activity\", \"end_activity\", \"type\", \"info\", \"adjacency_list\"]]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.022705Z",
     "start_time": "2021-05-29T18:44:47.008382Z"
    }
   },
   "outputs": [],
   "source": [
    "def strongly_connected_components(events):  \n",
    "    graph = nx.DiGraph()\n",
    "    for _, row in events.iterrows():\n",
    "        graph.add_node(row[\"id\"])\n",
    "        for connection in row[\"adjacency_list\"]:\n",
    "            graph.add_edge(row[\"id\"], connection)\n",
    "    \n",
    "    print(len(graph.edges()))\n",
    "    ccomponents = sorted([c for c in nx.weakly_connected_components(graph) if (len(c) > 1)], key=len)\n",
    "    ccomponents = [graph.subgraph(c).copy() for c in ccomponents]\n",
    "    \n",
    "    return ccomponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.038015Z",
     "start_time": "2021-05-29T18:44:47.023607Z"
    }
   },
   "outputs": [],
   "source": [
    "def paths(graph):\n",
    "    sources = [node for node, indegree in graph.in_degree(graph.nodes()) if indegree == 0]\n",
    "    sinks = [node for node, outdegree in graph.out_degree(graph.nodes()) if outdegree == 0]\n",
    "    paths = []\n",
    "    for (source, sink) in [(source, sink) for sink in sinks for source in sources]:\n",
    "        for path in nx.all_simple_paths(graph, source=source, target=sink):\n",
    "            paths.append(path)\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.054201Z",
     "start_time": "2021-05-29T18:44:47.038984Z"
    }
   },
   "outputs": [],
   "source": [
    "def cascades(events):\n",
    "    data = events.copy()\n",
    "    data[\"compact\"] = list(zip(data[\"id\"], data[\"start_activity\"] + \"|\" + data[\"end_activity\"], data[\"type\"], data[\"info\"]))\n",
    "    lookup = pd.Series(data[\"compact\"].values, index=data[\"id\"]).to_dict()\n",
    "    ccomponents = strongly_connected_components(data)\n",
    "    \n",
    "    cascades = []\n",
    "    for cc in ccomponents:\n",
    "        p = [list(map(lambda x: lookup[x], path)) for path in paths(cc)]\n",
    "        cascades += p\n",
    "        \n",
    "    return cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:46:40.330895Z",
     "start_time": "2021-05-29T18:46:40.324935Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_cascades(events, name, segment_groups, criteria=\"strict\", case_overlap=5):\n",
    "    cascade_list = []\n",
    "    for i, segments in enumerate(segment_groups):\n",
    "        data = filter_system_level_events(events.copy(), segments)\n",
    "        data_ext = add_adjacency_list(data, criteria, case_overlap)\n",
    "        c = cascades(data_ext)\n",
    "        print(i, len(c))\n",
    "        cascade_list += c\n",
    "    \n",
    "    save_cascades(cascade_list, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.074073Z",
     "start_time": "2021-05-29T18:44:46.802Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_subsequence(query, base):\n",
    "    l = len(query)\n",
    "\n",
    "    for i in range(len(base) - l + 1):\n",
    "        if base[i:i+l] == query:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.075070Z",
     "start_time": "2021-05-29T18:44:46.803Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_cascades(cascades, target):\n",
    "    data = list(map(lambda c: list(map(lambda x: (x[0], x[1]), c)), cascades))\n",
    "    filtered = list(filter(lambda c: is_subsequence(target, c), data))\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup(cascades):\n",
    "    data = list(map(lambda c: list(map(lambda x: (x[0], (x[1] + \"|\" + x[2])), c)), cascades))\n",
    "    flattened = [item for sublist in data for item in sublist]\n",
    "    lookup = dict(flattened)\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T22:07:07.356780Z",
     "start_time": "2021-05-29T22:07:07.336265Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frequencies(cascades, pattern):\n",
    "    print(pattern)\n",
    "    for subset in cascades.keys():\n",
    "        print(subset + \":\", end=\" \")\n",
    "        lookup = create_lookup(cascades[subset])\n",
    "        ids = set([k for k,v in lookup.items() if v in pattern])\n",
    "        \n",
    "        data = list(map(lambda c: list(map(lambda x: x[0], c)), cascades[subset]))\n",
    "        data = list(map(lambda x: tuple(set(x).intersection(ids)), data))\n",
    "        data = list(filter(lambda x: len(x) == len(pattern), data))\n",
    "        data = list(set(data))\n",
    "        print(len(data), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:25:33.823791Z",
     "start_time": "2021-06-04T15:25:33.811827Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clean_cascades(cascades, pattern):\n",
    "    lookup = create_lookup(cascades)\n",
    "    ids = set([k for k,v in lookup.items() if v in pattern])\n",
    "\n",
    "    data = list(map(lambda c: list(map(lambda x: x[0], c)), cascades))\n",
    "    data = list(map(lambda x: tuple(set(x).intersection(ids)), data))\n",
    "    data = list(filter(lambda x: len(x) == len(pattern), data))\n",
    "    data = list(set(data))\n",
    "    data = [sorted(list(c)) for c in data]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.076069Z",
     "start_time": "2021-05-29T18:44:46.805Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_cascades(cascades, name):\n",
    "    with open('output/dumps/cascades_' + name, 'wb+') as output:\n",
    "        pickle.dump(cascades, output, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.077065Z",
     "start_time": "2021-05-29T18:44:46.806Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_cascades(names):\n",
    "    cascades = {}\n",
    "    for name in names:\n",
    "        c = pickle.load(open('output/dumps/cascades_' + name, 'rb'))\n",
    "        cascades[name] = c\n",
    "    \n",
    "    return cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.077065Z",
     "start_time": "2021-05-29T18:44:46.807Z"
    }
   },
   "outputs": [],
   "source": [
    "def patterns(cascades, count=10):\n",
    "    df = pd.DataFrame([zip(*row) for row in cascades], columns=[\"ids\", \"cascades\", \"event_types\", \"values\"])\n",
    "    \n",
    "    temp = df.groupby(['cascades', 'event_types'])['values']\n",
    "\n",
    "    df = pd.concat(\n",
    "        (\n",
    "            temp.apply(lambda s: np.array(list(s)).mean(axis=0)),\n",
    "            temp.size()\n",
    "        ), axis=1,\n",
    "        keys=['values', 'count']\n",
    "    ).reset_index()\n",
    "    \n",
    "    df[\"values\"] = df[\"values\"].apply(lambda l: [round(x, 2) for x in l])\n",
    "\n",
    "    df[\"cascades\"] = df[\"cascades\"].apply(lambda x: list(x))\n",
    "    df[\"cascades\"] = df[\"cascades\"].apply(lambda x: [v for v, _ in groupby(chain.from_iterable(v.split(\"|\") for v in x))])\n",
    "    \n",
    "    df[\"event_types\"] = df[\"event_types\"].apply(lambda x: list(x))\n",
    "    \n",
    "    df[\"len\"] = df[\"event_types\"].apply(lambda x: len(x)).astype(int)\n",
    "\n",
    "    df[\"bat\"] = df[\"event_types\"].apply(lambda x: x.count(\"BAT\") + x.count(\"BST\") + x.count(\"BEN\")).astype(int)\n",
    "    df[\"wl\"] = df[\"event_types\"].apply(lambda x: x.count(\"WLD\") + x.count(\"WLH\")).astype(int)\n",
    "    df[\"wlh\"] = df[\"event_types\"].apply(lambda x: x.count(\"WLH\")).astype(int)\n",
    "    df[\"wld\"] = df[\"event_types\"].apply(lambda x: x.count(\"WLD\")).astype(int)\n",
    "    df[\"del\"] = df[\"event_types\"].apply(lambda x: x.count(\"DEL\")).astype(int)\n",
    "    df[\"_bat\"] = df[\"event_types\"].apply(lambda x: x.count(\"BAT\")).astype(int)\n",
    "    df[\"bst\"] = df[\"event_types\"].apply(lambda x: x.count(\"BST\")).astype(int)\n",
    "    df[\"ben\"] = df[\"event_types\"].apply(lambda x: x.count(\"BEN\")).astype(int)\n",
    "\n",
    "    df[\"bat\"] = df[\"bat\"] / df[\"len\"]\n",
    "    df[\"non_bat\"] = 1 - df[\"bat\"]\n",
    "    df[\"wl\"] = df[\"wl\"] / df[\"len\"]\n",
    "    df[\"wld\"] = df[\"wld\"] / df[\"len\"]\n",
    "    df[\"wlh\"] = df[\"wlh\"] / df[\"len\"]\n",
    "    df[\"del\"] = df[\"del\"] / df[\"len\"]\n",
    "    df[\"_bat\"] = df[\"_bat\"] / df[\"len\"]\n",
    "    df[\"bst\"] = df[\"bst\"] / df[\"len\"]\n",
    "    df[\"ben\"] = df[\"ben\"] / df[\"len\"]\n",
    "    \n",
    "    df = df[df[\"count\"] >= count].reset_index()\n",
    "    df = df[[\"cascades\", \"event_types\", \"count\", \"len\"]]\n",
    "    \n",
    "#     print(\"DEL\", mean(df[\"del\"].tolist()))\n",
    "#     print(\"BAT\", mean(df[\"_bat\"].tolist()))\n",
    "#     print(\"WLH\", mean(df[\"wlh\"].tolist()))\n",
    "#     print(\"WLD\", mean(df[\"wld\"].tolist()))\n",
    "#     print(\"BST\", mean(df[\"bst\"].tolist()))\n",
    "#     print(\"BEN\", mean(df[\"ben\"].tolist()))\n",
    "#     print(len(df))\n",
    "    \n",
    "#     segs = df[\"cascades\"].tolist()\n",
    "#     segs = [item for sublist in segs for item in sublist]\n",
    "#     inv_lookup = {v:k for k,v in Short.ACTIVITY.items()}\n",
    "#     segs = [inv_lookup[s] for s in segs]\n",
    "#     cnt = Counter(segs)\n",
    "#     labels, values = zip(*cnt.most_common())\n",
    "\n",
    "#     indexes = np.arange(len(labels))\n",
    "#     width = 1\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(11,11))\n",
    "  \n",
    "#     fig.savefig(\"heatmaps/\" + \"barch\" + \".png\", dpi=400)\n",
    "#     plt.bar(indexes, values, width)\n",
    "#     plt.xticks(indexes + width * 0.5, labels, rotation='vertical')\n",
    "#     plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.078062Z",
     "start_time": "2021-05-29T18:44:46.809Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_graphs(patterns):\n",
    "    pats = patterns.copy()\n",
    "    graphs = []\n",
    "    inv_lookup = {v:k for k,v in Short.ACTIVITY.items()}\n",
    "\n",
    "    for _, row in pats.iterrows():\n",
    "        graph = nx.DiGraph()\n",
    "        nodes = [inv_lookup[node] for node in row[\"cascades\"]]\n",
    "        edges = [(i, j) for i, j in zip(nodes, nodes[1:])]\n",
    "        edge_labels = [t + \"|\" + str(v) for (t, v) in zip(row[\"event_types\"], row[\"values\"])]\n",
    "\n",
    "        for node in nodes:\n",
    "            graph.add_node(node)\n",
    "        for i, edge in enumerate(edges):\n",
    "            graph.add_edge(edge[0], edge[1], label= \" \" + edge_labels[i])\n",
    "\n",
    "        graph = nx.nx_agraph.to_agraph(graph)\n",
    "        graphs.append(graph)\n",
    "\n",
    "    for i, graph in enumerate(graphs):\n",
    "        graph.layout('dot')            \n",
    "        graph.draw('patterns2/pattern' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.079060Z",
     "start_time": "2021-05-29T18:44:46.810Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_N_pattern(events):\n",
    "    data = events.copy()\n",
    "    patterns = []\n",
    "    \n",
    "    for segment in Segments.SEGMENTS_COMPLETE:\n",
    "        start_activity, end_activity = segment.split(\" - \", 1)\n",
    "\n",
    "        subset = data[(data[\"start_activity\"] == Short.ACTIVITY[start_activity]) & (data[\"end_activity\"] == Short.ACTIVITY[end_activity])]\n",
    "        if len(subset):\n",
    "            dels = subset[subset[\"duration\"] > 0]\n",
    "            instants = subset[subset[\"duration\"] == 0]\n",
    "            \n",
    "            for i, row in dels.iterrows():\n",
    "                start = row[\"start_time\"]\n",
    "                end = row[\"end_time\"]\n",
    "                instant_start = instants[instants[\"start_time\"] == row[\"start_time\"]]\n",
    "                instant_end = instants[instants[\"start_time\"] == row[\"end_time\"]]\n",
    "                if (len(instant_start) > 0 and len(instant_end) > 0):\n",
    "                    print(segment, row[\"start_time\"], row[\"end_time\"])\n",
    "                    pattern = pd.concat([instant_start, subset[(subset[\"start_time\"] == start) & (subset[\"end_time\"] == end)], instant_end])\n",
    "                    patterns.append(pattern)\n",
    "                \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.080068Z",
     "start_time": "2021-05-29T18:44:46.811Z"
    }
   },
   "outputs": [],
   "source": [
    "def N_patterns_graph(patterns):\n",
    "    inv_lookup = {v:k for k,v in Short.ACTIVITY.items()}\n",
    "    graphs = []\n",
    "    for pattern in patterns:\n",
    "        start_a = inv_lookup[pattern.iloc[0][\"start_activity\"]]\n",
    "        end_a = inv_lookup[pattern.iloc[0][\"end_activity\"]]\n",
    "        start_t = pattern.iloc[1][\"start_time\"]\n",
    "        end_t = pattern.iloc[1][\"end_time\"]\n",
    "        \n",
    "        graph = nx.DiGraph()\n",
    "        \n",
    "        nodes = [start_a + \" \" + start_t,\n",
    "                 end_a + \" \" + start_t,\n",
    "                 start_a + \" \" + end_t,\n",
    "                 end_a + \" \" + end_t]\n",
    "        \n",
    "        edge_labels = [\"dur: \" + str(d) + \", #cases: \" + str(c) for (d, c) in list(zip(pattern[\"duration\"].tolist(), pattern[\"nr_cases\"].tolist()))]\n",
    "\n",
    "        for node in nodes:\n",
    "            graph.add_node(node)\n",
    "        for i, edge in enumerate([(nodes[0], nodes[1]), (nodes[0], nodes[3]), (nodes[2], nodes[3])]):\n",
    "            graph.add_edge(edge[0], edge[1], label= \" \" + edge_labels[i])\n",
    "\n",
    "        graph = nx.nx_agraph.to_agraph(graph)\n",
    "        graphs.append(graph)\n",
    "        \n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-29T18:44:47.081065Z",
     "start_time": "2021-05-29T18:44:46.812Z"
    }
   },
   "outputs": [],
   "source": [
    "def batches(event_log, segments):\n",
    "    sys_events = []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        seg_events = segment_level_events(event_log, segment)\n",
    "\n",
    "        events = system_level_events_delayed(seg_events, treshold_occurence=53)\n",
    "\n",
    "        events[\"start_date\"] = list(zip(events[\"start_time\"], events[\"start_time\"]))\n",
    "        events[\"end_date\"] = list(zip(events[\"end_time\"], events[\"end_time\"]))\n",
    "        startdates = events[\"start_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").strftime(\"%y%m%d\"))\n",
    "        enddates = events[\"end_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").strftime(\"%y%m%d\"))\n",
    "        start = events[\"start_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "        start = start - FIRST\n",
    "        end = events[\"end_time\"].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "        end = end - FIRST\n",
    "        events[\"date_label\"] = startdates.astype(str) + \"-\" + enddates.astype(str)\n",
    "\n",
    "        if not events.empty:\n",
    "            delays = events[events[\"duration\"] > 0]\n",
    "            if len(delays) > 0:\n",
    "                sys_events.append(events)        \n",
    "    \n",
    "    if sys_events:\n",
    "        data = pd.concat(sys_events)\n",
    "        new = data[\"segment_name\"].str.split(\" - \", n=1, expand=True)\n",
    "        data[\"start_activity\"] = new[0].replace(Short.ACTIVITY)\n",
    "        data[\"end_activity\"] = new[1].replace(Short.ACTIVITY)\n",
    "        data[\"nr_cases\"] = data[\"cases\"].str.len()\n",
    "    \n",
    "    data = data[[\"start_activity\", \"end_activity\", \"start_time\", \"end_time\", \"duration\", \"nr_cases\"]]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_statistics(event_log, cascade_list, pattern, cases, durations, index, scatter, hist):\n",
    "    ratios = []\n",
    "    for i, c in enumerate(cascade_list):\n",
    "        start_cases = set(cases[c[0]])\n",
    "        end_cases = set(cases[c[-1]])\n",
    "        overlap = (start_cases.intersection(end_cases))\n",
    "        ratios.append(len(overlap) / len(start_cases))\n",
    "    \n",
    "    if all(r >= 0.666 for r in ratios) or len(pattern) == 2:       \n",
    "        print(\"---------------------------------------\")\n",
    "        print(pattern)\n",
    "        print(\"---------------------------------------\")\n",
    "        not_pattern = cases_in_trace(event_log, pattern)\n",
    "        _pattern = set()\n",
    "        x = []\n",
    "        y = []\n",
    "        for i, c in enumerate(cascade_list):\n",
    "            start_cases = set(cases[c[0]])\n",
    "            end_cases = set(cases[c[-1]])\n",
    "            overlap = start_cases.intersection(end_cases)\n",
    "\n",
    "            not_pattern = not_pattern.difference(overlap)\n",
    "            _pattern = _pattern.union(overlap)\n",
    "            \n",
    "            if scatter:\n",
    "                x.append(len(overlap.intersection(set(successful_cases))))\n",
    "                y.append(len(overlap.intersection(set(not_successful_cases))))\n",
    "                \n",
    "        if scatter:\n",
    "            plt.scatter(x, y)\n",
    "            plt.title((pattern[0] + \"-\" + pattern[1]).replace(\"|\", \"-\"))\n",
    "            plt.xlabel(\"#successful\")\n",
    "            plt.xlim(0, 180)\n",
    "            plt.ylabel(\"#unsuccessful\")\n",
    "            plt.ylim(0, 180)\n",
    "            plt.savefig(\"output/scatterplots2/\" + str(index) + (pattern[0] + \"-\" + pattern[1]).replace(\"|\", \"-\") + \".jpg\")\n",
    "            plt.clf()\n",
    "            \n",
    "        if hist:\n",
    "            c = [durations[str(x)] for x in _pattern]\n",
    "            _pattern_med_dur = median([x for x in c if x > 30])\n",
    "            n, bins, patches = plt.hist(x=c, bins=range(0,150), alpha=0.7, rwidth=0.85)\n",
    "            plt.grid(axis='y', alpha=0.75)\n",
    "            plt.xlabel('Duration')\n",
    "            plt.xlim(0, 150)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title((pattern[0] + \"-\" + pattern[1]).replace(\"|\", \"-\"))\n",
    "            plt.savefig(\"output/histograms2/\" + str(index) + (pattern[0] + \"-\" + pattern[1]).replace(\"|\", \"-\") + \".jpg\")\n",
    "            plt.clf()\n",
    "            \n",
    "            c = [durations[str(x)] for x in not_pattern] \n",
    "            not_pattern_med_dur = median([x for x in c if x > 30])\n",
    "            n, bins, patches = plt.hist(x=c, bins=range(0,150), alpha=0.7, rwidth=0.85)\n",
    "            plt.grid(axis='y', alpha=0.75)\n",
    "            plt.xlabel('Duration')\n",
    "            plt.xlim(0, 150)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title((\"NOT\" + pattern[0] + \"-\" + pattern[1]).replace(\"|\", \"-\"))\n",
    "            plt.savefig(\"output/histograms2/NOT_\" + str(index) + (pattern[0] + \"-\" + pattern[1]).replace(\"|\", \"-\") + \".jpg\")\n",
    "            plt.clf()\n",
    "        \n",
    "        with open(\"output/cases2/\" + str(index) + (pattern[0] + \"-\" + pattern[1]).replace(\"|\", \"-\"), 'wb+') as output:\n",
    "            pickle.dump(_pattern, output, -1)\n",
    "        \n",
    "        with open(\"output/cases2/NOT_\" + str(index) + (pattern[0] + \"-\" + pattern[1]).replace(\"|\", \"-\"), 'wb+') as output:\n",
    "            pickle.dump(not_pattern, output, -1)\n",
    "            \n",
    "        _pattern_succ = len(_pattern.intersection(set(successful_cases)))\n",
    "        _pattern_nsucc = len(_pattern.intersection(set(not_successful_cases)))\n",
    "        _pattern_fast = len(_pattern.intersection(set(_10days_cases)))\n",
    "        _pattern_avg = len(_pattern.intersection(set(_1030days_cases)))\n",
    "        _pattern_slow = len(_pattern.intersection(set(_30days_cases)))\n",
    "        \n",
    "        _pattern_succ_prob = _pattern_succ / len(_pattern)\n",
    "        _pattern_nsucc_prob = _pattern_nsucc / len(_pattern)\n",
    "        _pattern_fast_prob = _pattern_fast / len(_pattern)\n",
    "        _pattern_avg_prob = _pattern_avg / len(_pattern)\n",
    "        _pattern_slow_prob = _pattern_slow / len(_pattern)\n",
    "        \n",
    "        not_pattern_succ = len(not_pattern.intersection(set(successful_cases)))\n",
    "        not_pattern_nsucc = len(not_pattern.intersection(set(not_successful_cases)))\n",
    "        not_pattern_fast = len(not_pattern.intersection(set(_10days_cases)))\n",
    "        not_pattern_avg = len(not_pattern.intersection(set(_1030days_cases)))\n",
    "        not_pattern_slow = len(not_pattern.intersection(set(_30days_cases)))\n",
    "        \n",
    "        not_pattern_succ_prob = not_pattern_succ / len(not_pattern)\n",
    "        not_pattern_nsucc_prob = not_pattern_nsucc / len(not_pattern)\n",
    "        not_pattern_fast_prob = not_pattern_fast / len(not_pattern)\n",
    "        not_pattern_avg_prob = not_pattern_avg / len(not_pattern)\n",
    "        not_pattern_slow_prob = not_pattern_slow / len(not_pattern)\n",
    "        \n",
    "        succ_sign = get_significance([_pattern_succ, _pattern_nsucc], [not_pattern_succ, not_pattern_nsucc])\n",
    "        dur_sign = get_significance([_pattern_fast, _pattern_avg, _pattern_slow], [not_pattern_fast, not_pattern_avg, not_pattern_slow])\n",
    "\n",
    "        print(len(cascade_list), \"CASCADES\")\n",
    "        print(\"PATTERN:     SUCC\", _pattern_succ, \"(\" + str(round(_pattern_succ_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"NSUCC\", _pattern_nsucc, \"(\" + str(round(_pattern_nsucc_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"FAST\", _pattern_fast, \"(\" + str(round(_pattern_fast_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"AVG\", _pattern_avg, \"(\" + str(round(_pattern_avg_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"SLOW\", _pattern_slow, \"(\" + str(round(_pattern_slow_prob*100, 1)), end=\"%) (\")\n",
    "        print(\"MED\", _pattern_med_dur, end=\")| \") \n",
    "        print(\"(\", len(_pattern), \"CASES )\")\n",
    "        \n",
    "        print(\"NOT PATTERN: SUCC\", not_pattern_succ, \"(\" + str(round(not_pattern_succ_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"NSUCC\", not_pattern_nsucc, \"(\" + str(round(not_pattern_nsucc_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"FAST\", not_pattern_fast, \"(\" + str(round(not_pattern_fast_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"AVG\", not_pattern_avg, \"(\" + str(round(not_pattern_avg_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"SLOW\", not_pattern_slow, \"(\" + str(round(not_pattern_slow_prob*100, 1)), end=\"%) (\")\n",
    "        print(\"MED\", not_pattern_med_dur, end=\")| \") \n",
    "        print(\"(\", len(not_pattern), \"CASES )\")\n",
    "        \n",
    "        print((\"SUCC SIGN: YES\" if succ_sign < 0.05 else \"SUCC SIGN: NO\"), end =\" \") \n",
    "        print(\"(p-value: \" + str(succ_sign) + \")\")\n",
    "        print((\"DUR SIGN: YES\" if dur_sign < 0.05 else \"DUR SIGN: NO\"), end =\" \") \n",
    "        print(\"(p-value: \" + str(dur_sign) + \")\")       \n",
    "    else:      \n",
    "        for i, j in zip(range(len(pattern)), range(len(pattern))[1:]):\n",
    "            new_pattern = [pattern[i], pattern[j]]\n",
    "            pattern_statistics(event_log, [[c[i], c[j]] for c in cascade_list], new_pattern, cases, durations, index, scatter, hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_statistics2(event_log, cascade_list, pattern, cases, durations, index, data, first=True):\n",
    "    ratios = []\n",
    "    df = data.copy()\n",
    "    for i, c in enumerate(cascade_list):\n",
    "        start_cases = set(cases[c[0]])\n",
    "        end_cases = set(cases[c[-1]])\n",
    "        overlap = (start_cases.intersection(end_cases))\n",
    "        ratios.append(len(overlap) / len(start_cases))\n",
    "    \n",
    "    if all(r >= 0.666 for r in ratios) or len(pattern) == 2:\n",
    "        if first:\n",
    "            print(\"||\", index, \"||\")\n",
    "            df.at[index, 'maximal'] = True\n",
    "        print(\"---------------------------------------\")\n",
    "        print(pattern)\n",
    "        print(\"---------------------------------------\")\n",
    "        not_pattern = cases_in_trace(event_log, pattern)\n",
    "        _pattern = set()\n",
    "\n",
    "        for i, c in enumerate(cascade_list):\n",
    "            start_cases = set(cases[c[0]])\n",
    "            end_cases = set(cases[c[-1]])\n",
    "            overlap = start_cases.intersection(end_cases)\n",
    "\n",
    "            not_pattern = not_pattern.difference(overlap)\n",
    "            _pattern = _pattern.union(overlap)\n",
    "                \n",
    "        c = [durations[str(x)] for x in _pattern]\n",
    "        _pattern_med_dur = median([x for x in c if x > 30])\n",
    "        c = [durations[str(x)] for x in not_pattern] \n",
    "        not_pattern_med_dur = median([x for x in c if x > 30])\n",
    "            \n",
    "        _pattern_succ = len(_pattern.intersection(set(successful_cases)))\n",
    "        _pattern_nsucc = len(_pattern.intersection(set(not_successful_cases)))\n",
    "        _pattern_fast = len(_pattern.intersection(set(_10days_cases)))\n",
    "        _pattern_avg = len(_pattern.intersection(set(_1030days_cases)))\n",
    "        _pattern_slow = len(_pattern.intersection(set(_30days_cases)))\n",
    "        \n",
    "        _pattern_succ_prob = _pattern_succ / len(_pattern)\n",
    "        _pattern_nsucc_prob = _pattern_nsucc / len(_pattern)\n",
    "        _pattern_fast_prob = _pattern_fast / len(_pattern)\n",
    "        _pattern_avg_prob = _pattern_avg / len(_pattern)\n",
    "        _pattern_slow_prob = _pattern_slow / len(_pattern)\n",
    "        \n",
    "        not_pattern_succ = len(not_pattern.intersection(set(successful_cases)))\n",
    "        not_pattern_nsucc = len(not_pattern.intersection(set(not_successful_cases)))\n",
    "        not_pattern_fast = len(not_pattern.intersection(set(_10days_cases)))\n",
    "        not_pattern_avg = len(not_pattern.intersection(set(_1030days_cases)))\n",
    "        not_pattern_slow = len(not_pattern.intersection(set(_30days_cases)))\n",
    "        \n",
    "        not_pattern_succ_prob = not_pattern_succ / len(not_pattern)\n",
    "        not_pattern_nsucc_prob = not_pattern_nsucc / len(not_pattern)\n",
    "        not_pattern_fast_prob = not_pattern_fast / len(not_pattern)\n",
    "        not_pattern_avg_prob = not_pattern_avg / len(not_pattern)\n",
    "        not_pattern_slow_prob = not_pattern_slow / len(not_pattern)\n",
    "        \n",
    "        succ_sign = get_significance([_pattern_succ, _pattern_nsucc], [not_pattern_succ, not_pattern_nsucc])\n",
    "        dur_sign = get_significance([_pattern_fast, _pattern_avg, _pattern_slow], [not_pattern_fast, not_pattern_avg, not_pattern_slow])\n",
    "\n",
    "        print(len(cascade_list), \"CASCADES\")\n",
    "        print(\"PATTERN:     SUCC\", _pattern_succ, \"(\" + str(round(_pattern_succ_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"NSUCC\", _pattern_nsucc, \"(\" + str(round(_pattern_nsucc_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"FAST\", _pattern_fast, \"(\" + str(round(_pattern_fast_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"AVG\", _pattern_avg, \"(\" + str(round(_pattern_avg_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"SLOW\", _pattern_slow, \"(\" + str(round(_pattern_slow_prob*100, 1)), end=\"%) (\")\n",
    "        print(\"MED\", _pattern_med_dur, end=\")| \") \n",
    "        print(\"(\", len(_pattern), \"CASES )\")\n",
    "        \n",
    "        print(\"NOT PATTERN: SUCC\", not_pattern_succ, \"(\" + str(round(not_pattern_succ_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"NSUCC\", not_pattern_nsucc, \"(\" + str(round(not_pattern_nsucc_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"FAST\", not_pattern_fast, \"(\" + str(round(not_pattern_fast_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"AVG\", not_pattern_avg, \"(\" + str(round(not_pattern_avg_prob*100, 1)), end=\"%)| \")\n",
    "        print(\"SLOW\", not_pattern_slow, \"(\" + str(round(not_pattern_slow_prob*100, 1)), end=\"%) (\")\n",
    "        print(\"MED\", not_pattern_med_dur, end=\")| \") \n",
    "        print(\"(\", len(not_pattern), \"CASES )\")\n",
    "        \n",
    "        print((\"SUCC SIGN: YES\" if succ_sign < 0.05 else \"SUCC SIGN: NO\"), end =\" \") \n",
    "        print(\"(p-value: \" + str(succ_sign) + \")\")\n",
    "        print((\"DUR SIGN: YES\" if dur_sign < 0.05 else \"DUR SIGN: NO\"), end =\" \") \n",
    "        print(\"(p-value: \" + str(dur_sign) + \")\")\n",
    "        \n",
    "        if first:\n",
    "            df.at[index, \"nr_cases\"] = len(_pattern)\n",
    "    else:      \n",
    "        for i, j in zip(range(len(pattern)), range(len(pattern))[1:]):\n",
    "            new_pattern = [pattern[i], pattern[j]]\n",
    "            pattern_statistics2(event_log, [[c[i], c[j]] for c in cascade_list], new_pattern, cases, durations, index, df, first=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significance(pattern_list, not_pattern_list):\n",
    "    stats = importr('stats')\n",
    "    m = np.array([pattern_list, not_pattern_list])\n",
    "    res = stats.fisher_test(m, simulate_p_value=True, conf_level = 0.95)\n",
    "\n",
    "    return(res[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_patterns_statistics(event_log, events, raw_cascades, patterns, scatter=False, hist=False):\n",
    "    cases = pd.Series(events[\"complete\"][\"cases\"].values, index=events[\"complete\"][\"id\"]).to_dict()\n",
    "    durations = pickle.load(open('output/dumps/durations', 'rb'))\n",
    "    \n",
    "    for i, pattern in enumerate(patterns):\n",
    "        print(\"_________________________________________\")\n",
    "        print(\"Complete pattern:\", pattern)\n",
    "        cascade_list = get_clean_cascades(raw_cascades, pattern)\n",
    "        pattern_statistics(event_log, cascade_list, pattern, cases, durations, i, scatter, hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_patterns_statistics2(event_log, events, raw_cascades, patterns):\n",
    "    cases = pd.Series(events[\"complete\"][\"cases\"].values, index=events[\"complete\"][\"id\"]).to_dict()\n",
    "    durations = pickle.load(open('output/dumps/durations', 'rb'))\n",
    "    df = patterns.copy()\n",
    "    df[\"nr_cases\"] = 0\n",
    "    df[\"maximal\"] = False\n",
    "    \n",
    "    for i in range(len(patterns)):\n",
    "        pattern_str = pattern_string(patterns, i)\n",
    "        print(\"_________________________________________\")\n",
    "        print(i, \"Complete pattern:\", pattern_str)\n",
    "        \n",
    "        cascade_list = get_clean_cascades(raw_cascades, pattern_str)\n",
    "        df = pattern_statistics2(event_log, cascade_list, pattern_str, cases, durations, i, df, True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cases_in_trace(event_log, pattern):\n",
    "    inv_lookup = {v:k for k,v in Short.ACTIVITY.items()}\n",
    "    segments = list(map(lambda x: inv_lookup[x.split(\"|\")[0]] + \" - \" + inv_lookup[x.split(\"|\")[1]], pattern))\n",
    "    cases_list = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        seg_events = segment_level_events(event_log, segment)\n",
    "        cases = set(seg_events[\"case_id\"].tolist())\n",
    "        cases_list.append(cases)\n",
    "\n",
    "    inter = set.intersection(*cases_list)\n",
    "\n",
    "    return inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_findings(event_log, subset_cases, segments):\n",
    "    print(segments)\n",
    "    cases_sets = []\n",
    "    for segment in segments:\n",
    "        segs = segment_level_events(event_log, segment)\n",
    "        cases_set = set(segs[\"case_id\"].tolist())\n",
    "        cases_sets.append(cases_set)\n",
    "        \n",
    "    segments_subset = set.intersection(*cases_sets)\n",
    "    print(\"#cases in subset:\", len(subset_cases))\n",
    "    \n",
    "#     segments_subset = set(subset_cases).intersection(segments_cases)\n",
    "    print(\"#cases containing sequence:\", len(segments_subset))\n",
    "    segments_succ = len(segments_subset.intersection(successful_cases))\n",
    "    segments_succ_ratio = round(segments_succ / len(segments_subset) * 100, 1)\n",
    "    segments_nsucc = len(segments_subset.intersection(not_successful_cases))\n",
    "    segments_nsucc_ratio = round(segments_nsucc / len(segments_subset) * 100, 1)\n",
    "    \n",
    "    segments_fast = len(segments_subset.intersection(_10days_cases))\n",
    "    segments_fast_ratio = round(segments_fast / len(segments_subset) * 100, 1)\n",
    "    segments_av = len(segments_subset.intersection(_1030days_cases))\n",
    "    segments_av_ratio = round(segments_av / len(segments_subset) * 100, 1)\n",
    "    segments_slow = len(segments_subset.intersection(_30days_cases))\n",
    "    segments_slow_ratio = round(segments_slow / len(segments_subset) * 100, 1)\n",
    "    \n",
    "    segments_med_dur = median([durations[str(x)] for x in segments_subset])\n",
    "    print('SUCC', segments_succ, \"(\" + str(segments_succ_ratio) + \"%) | NSUCC\", segments_nsucc, \"(\" + str(segments_nsucc_ratio) + '%) | FAST', segments_fast, \"(\" + str(segments_fast_ratio) + '%) | AV', segments_av, \"(\" + str(segments_av_ratio) + '%) | SLOW', segments_slow, \"(\" + str(segments_slow_ratio) + \"%) | MED DUR\", segments_med_dur)\n",
    "    \n",
    "    not_segments_subset = set(all_cases).difference(segments_subset)\n",
    "    print(\"#cases not containing sequence:\", len(not_segments_subset))\n",
    "    not_segments_succ = len(not_segments_subset.intersection(successful_cases))\n",
    "    not_segments_succ_ratio = round(not_segments_succ / len(not_segments_subset) * 100, 1)\n",
    "    not_segments_nsucc = len(not_segments_subset.intersection(not_successful_cases))\n",
    "    not_segments_nsucc_ratio = round(not_segments_nsucc / len(not_segments_subset) * 100, 1)\n",
    "    \n",
    "    not_segments_fast = len(not_segments_subset.intersection(_10days_cases))\n",
    "    not_segments_fast_ratio = round(not_segments_fast / len(not_segments_subset) * 100, 1)\n",
    "    not_segments_av = len(not_segments_subset.intersection(_1030days_cases))\n",
    "    not_segments_av_ratio = round(not_segments_av / len(not_segments_subset) * 100, 1)\n",
    "    not_segments_slow = len(not_segments_subset.intersection(_30days_cases))\n",
    "    not_segments_slow_ratio = round(not_segments_slow / len(not_segments_subset) * 100, 1)\n",
    "    \n",
    "    not_segments_med_dur = median([durations[str(x)] for x in not_segments_subset])\n",
    "    print('SUCC', not_segments_succ, \"(\" + str(not_segments_succ_ratio) + \"%) | NSUCC\", not_segments_nsucc, \"(\" + str(not_segments_nsucc_ratio) + '%) | FAST', not_segments_fast, \"(\" + str(not_segments_fast_ratio) + '%) | AV', not_segments_av, \"(\" + str(not_segments_av_ratio) + '%) | SLOW', not_segments_slow, \"(\" + str(not_segments_slow_ratio) + \"%) | MED DUR\", not_segments_med_dur)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_string(patterns, index):\n",
    "    p = patterns.copy()\n",
    "    p[\"cascades\"] = p[\"cascades\"].apply(lambda activities: [(i + \"|\" + j) for i, j in zip(activities, activities[1:])])\n",
    "    p[\"cascades\"] = p.apply(lambda x: list(zip(x[\"cascades\"], x[\"event_types\"])), axis=1)\n",
    "    p[\"cascades\"] = p[\"cascades\"].apply(lambda pairs: [(pair[0] + \"|\" + pair[1]) for pair in pairs])\n",
    "    \n",
    "    return p[\"cascades\"].tolist()[index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
